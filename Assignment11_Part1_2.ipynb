{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MPGarg/Assignment-11/blob/main/Assignment11_Part1_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfF0KNxIs-G7",
        "outputId": "3b367e06-8838-449e-f364-465c3ecce5da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/MPGarg/Assignment-11.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwvwhXdytTj3",
        "outputId": "eaa9feda-4d16-4207-ec3e-2749f7f38926"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Assignment-11'...\n",
            "remote: Enumerating objects: 13, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 13 (delta 3), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (13/13), 1.92 MiB | 2.65 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClxtPkcbtod-",
        "outputId": "0fc4da5c-8f72-4aaf-8805-50bbc1f7255b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mAssignment-11\u001b[0m/  \u001b[01;34mgdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading vocab & training file\n",
        "!cp -r '/content/gdrive/MyDrive/Colab Notebooks/Assignment11/vocab.txt' /content/\n",
        "!cp -r '/content/gdrive/MyDrive/Colab Notebooks/Assignment11/training.txt' /content/"
      ],
      "metadata": {
        "id": "UqjWAVRBt9Jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3iAVOAat9MX",
        "outputId": "21ae811e-bded-4e32-cb80-6ae6c21b1dc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mAssignment-11\u001b[0m/  names.tsv     training.txt  vocab.txt\n",
            "\u001b[01;34mgdrive\u001b[0m/         \u001b[01;34msample_data\u001b[0m/  values.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoLbyA1qvWRc",
        "outputId": "5951b483-47cc-4959-e0aa-d71368f70acb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYq3Smi6sn_S"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Libs\n",
        "# =============================================================================\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "from os.path import exists\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import math\n",
        "import re\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Transformer\n",
        "# =============================================================================\n",
        "def attention(q, k, v, mask = None, dropout = None):\n",
        "    scores = q.matmul(k.transpose(-2, -1))\n",
        "    scores /= math.sqrt(q.shape[-1])\n",
        "    \n",
        "    #mask\n",
        "    scores = scores if mask is None else scores.masked_fill(mask == 0, -1e3)\n",
        "    \n",
        "    scores = F.softmax(scores, dim = -1)\n",
        "    scores = dropout(scores) if dropout is not None else scores\n",
        "    output = scores.matmul(v)\n",
        "    return output\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_heads, out_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "#        self.q_linear = nn.Linear(out_dim, out_dim)\n",
        "#        self.k_linear = nn.Linear(out_dim, out_dim)\n",
        "#        self.v_linear = nn.Linear(out_dim, out_dim)\n",
        "        self.linear = nn.Linear(out_dim, out_dim*3)\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "        self.out_dim = out_dim\n",
        "        self.out_dim_per_head = out_dim // n_heads\n",
        "        self.out = nn.Linear(out_dim, out_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def split_heads(self, t):\n",
        "        return t.reshape(t.shape[0], -1, self.n_heads, self.out_dim_per_head)\n",
        "    \n",
        "    def forward(self, x, y=None, mask=None):\n",
        "        #in decoder, y comes from encoder. In encoder, y=x\n",
        "        y = x if y is None else y\n",
        "        \n",
        "        qkv = self.linear(x) # BS * SEQ_LEN * (3*EMBED_SIZE_L)\n",
        "        q = qkv[:, :, :self.out_dim] # BS * SEQ_LEN * EMBED_SIZE_L\n",
        "        k = qkv[:, :, self.out_dim:self.out_dim*2] # BS * SEQ_LEN * EMBED_SIZE_L\n",
        "        v = qkv[:, :, self.out_dim*2:] # BS * SEQ_LEN * EMBED_SIZE_L\n",
        "        \n",
        "        #break into n_heads\n",
        "        q, k, v = [self.split_heads(t) for t in (q,k,v)]  # BS * SEQ_LEN * HEAD * EMBED_SIZE_P_HEAD\n",
        "        q, k, v = [t.transpose(1,2) for t in (q,k,v)]  # BS * HEAD * SEQ_LEN * EMBED_SIZE_P_HEAD\n",
        "        \n",
        "        #n_heads => attention => merge the heads => mix information\n",
        "        scores = attention(q, k, v, mask, self.dropout) # BS * HEAD * SEQ_LEN * EMBED_SIZE_P_HEAD\n",
        "        scores = scores.transpose(1,2).contiguous().view(scores.shape[0], -1, self.out_dim) # BS * SEQ_LEN * EMBED_SIZE_L\n",
        "        out = self.out(scores)  # BS * SEQ_LEN * EMBED_SIZE\n",
        "        \n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, inp_dim, inner_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(inp_dim, inner_dim)\n",
        "        self.linear2 = nn.Linear(inner_dim, inp_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        #inp => inner => relu => dropout => inner => inp\n",
        "        return self.linear2(self.dropout(F.relu(self.linear1(x)))) \n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, n_heads, inner_transformer_size, inner_ff_size, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(n_heads, inner_transformer_size, dropout)\n",
        "        self.ff = FeedForward(inner_transformer_size, inner_ff_size, dropout)\n",
        "        self.norm1 = nn.LayerNorm(inner_transformer_size)\n",
        "        self.norm2 = nn.LayerNorm(inner_transformer_size)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        x2 = self.norm1(x)\n",
        "        x = x + self.dropout1(self.mha(x2, mask=mask))\n",
        "        x2 = self.norm2(x)\n",
        "        x = x + self.dropout2(self.ff(x2))\n",
        "        return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, n_code, n_heads, embed_size, inner_ff_size, n_embeddings, seq_len, dropout=.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        #model input\n",
        "        self.embeddings = nn.Embedding(n_embeddings, embed_size)\n",
        "        self.pe = PositionalEmbedding(embed_size, seq_len)\n",
        "        \n",
        "        #backbone\n",
        "        encoders = []\n",
        "        for i in range(n_code):\n",
        "            encoders += [EncoderLayer(n_heads, embed_size, inner_ff_size, dropout)]\n",
        "        self.encoders = nn.ModuleList(encoders)\n",
        "        \n",
        "        #language model\n",
        "        self.norm = nn.LayerNorm(embed_size)\n",
        "        self.linear = nn.Linear(embed_size, n_embeddings, bias=False)\n",
        "                \n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.embeddings(x)\n",
        "        x = x + self.pe(x)\n",
        "        for encoder in self.encoders:\n",
        "            x = encoder(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "# Positional Embedding\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len = 80):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        pe.requires_grad = False\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.pe[:,:x.size(1)] #x.size(1) = seq_len\n",
        "    \n",
        "# =============================================================================\n",
        "# Dataset\n",
        "# =============================================================================\n",
        "class SentencesDataset(Dataset):\n",
        "    #Init dataset\n",
        "    def __init__(self, sentences, vocab, seq_len):\n",
        "        dataset = self\n",
        "        \n",
        "        dataset.sentences = sentences\n",
        "        dataset.vocab = vocab + ['<ignore>', '<oov>', '<mask>', 'samsung']\n",
        "        dataset.vocab = {e:i for i, e in enumerate(dataset.vocab)} \n",
        "        dataset.rvocab = {v:k for k,v in dataset.vocab.items()}\n",
        "        dataset.seq_len = seq_len\n",
        "        \n",
        "        #special tags\n",
        "        dataset.IGNORE_IDX = dataset.vocab['<ignore>'] #replacement tag for tokens to ignore\n",
        "        dataset.OUT_OF_VOCAB_IDX = dataset.vocab['<oov>'] #replacement tag for unknown words\n",
        "        dataset.MASK_IDX = dataset.vocab['<mask>'] #replacement tag for the masked word prediction task\n",
        "        dataset.WORD1_IDX = dataset.vocab['samsung']        \n",
        "    \n",
        "    #fetch data\n",
        "    def __getitem__(self, index, p_noisy=0.15):\n",
        "        dataset = self\n",
        "        \n",
        "        #while we don't have enough word to fill the sentence for a batch\n",
        "        s = []\n",
        "        while len(s) < dataset.seq_len:\n",
        "            s.extend(dataset.get_sentence_idx(index % len(dataset)))\n",
        "            index += 1\n",
        "        \n",
        "        #ensure that the sequence is of length seq_len\n",
        "        s = s[:dataset.seq_len]\n",
        "        [s.append(dataset.IGNORE_IDX) for i in range(dataset.seq_len - len(s))] #PAD ok\n",
        "        \n",
        "        #apply random word for mars\n",
        "        s = [(dataset.WORD1_IDX, w) if random.random() < p_noisy else (w, w) for w in s]\n",
        "        \n",
        "        return {'input': torch.Tensor([w[0] for w in s]).long(),\n",
        "                'target': torch.Tensor([w[1] for w in s]).long()}\n",
        "\n",
        "    #return length\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    #get words id\n",
        "    def get_sentence_idx(self, index):\n",
        "        dataset = self\n",
        "        s = dataset.sentences[index]\n",
        "        s = [dataset.vocab[w] if w in dataset.vocab else dataset.OUT_OF_VOCAB_IDX for w in s] \n",
        "        return s\n",
        "\n",
        "# =============================================================================\n",
        "# Methods / Class\n",
        "# =============================================================================\n",
        "def get_batch(loader, loader_iter):\n",
        "    try:\n",
        "        batch = next(loader_iter)\n",
        "    except StopIteration:\n",
        "        loader_iter = iter(loader)\n",
        "        batch = next(loader_iter)\n",
        "    return batch, loader_iter\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# #Init\n",
        "# =============================================================================\n",
        "print('initializing..')\n",
        "batch_size = 1024\n",
        "seq_len = 20\n",
        "embed_size = 128\n",
        "inner_ff_size = embed_size * 4\n",
        "n_heads = 8\n",
        "n_code = 8\n",
        "n_vocab = 40000\n",
        "dropout = 0.1\n",
        "# n_workers = 12\n",
        "\n",
        "#optimizer\n",
        "optim_kwargs = {'lr':1e-4, 'weight_decay':1e-4, 'betas':(.9,.999)}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REytZ7yCUzNz",
        "outputId": "40f139db-c528-46be-e64a-7c7fae3393dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initializing..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Input\n",
        "# =============================================================================\n",
        "#1) load text\n",
        "print('loading text...')\n",
        "pth = 'training.txt'\n",
        "sentences = open(pth).read().lower().split('\\n')\n",
        "\n",
        "#2) tokenize sentences (can be done during training, you can also use spacy udpipe)\n",
        "print('tokenizing sentences...')\n",
        "special_chars = ',?;.:/*!+-()[]{}\"\\'&'\n",
        "sentences = [re.sub(f'[{re.escape(special_chars)}]', ' \\g<0> ', s).split(' ') for s in sentences]\n",
        "sentences = [[w for w in s if len(w)] for s in sentences]\n",
        "\n",
        "#3) create vocab if not already created\n",
        "print('creating/loading vocab...')\n",
        "pth = 'vocab.txt'\n",
        "if not exists(pth):\n",
        "    words = [w for s in sentences for w in s]\n",
        "    vocab = Counter(words).most_common(n_vocab) #keep the N most frequent words\n",
        "    vocab = [w[0] for w in vocab]\n",
        "    open(pth, 'w+').write('\\n'.join(vocab))\n",
        "else:\n",
        "    vocab = open(pth).read().split('\\n')\n",
        "\n",
        "#4) create dataset\n",
        "print('creating dataset...')\n",
        "dataset = SentencesDataset(sentences, vocab, seq_len)\n",
        "# kwargs = {'num_workers':n_workers, 'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
        "kwargs = {'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
        "data_loader = torch.utils.data.DataLoader(dataset, **kwargs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HU-RAqXtUzQn",
        "outputId": "5536f897-b562-46e3-a4aa-e6f728d285ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading text...\n",
            "tokenizing sentences...\n",
            "creating/loading vocab...\n",
            "creating dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBurNy4hsn_Y",
        "outputId": "64772499-5cdd-4aad-d643-776114bbefe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initializing model...\n",
            "initializing optimizer and loss...\n",
            "training...\n",
            "it: 0  | loss 10.37  | Δw: 3.838\n",
            "it: 10  | loss 8.46  | Δw: 2.42\n",
            "it: 20  | loss 7.77  | Δw: 1.924\n",
            "it: 30  | loss 7.36  | Δw: 1.774\n",
            "it: 40  | loss 6.97  | Δw: 1.747\n",
            "it: 50  | loss 6.57  | Δw: 1.671\n",
            "it: 60  | loss 6.4  | Δw: 1.663\n",
            "it: 70  | loss 6.11  | Δw: 1.618\n",
            "it: 80  | loss 5.84  | Δw: 1.557\n",
            "it: 90  | loss 5.63  | Δw: 1.512\n",
            "it: 100  | loss 5.38  | Δw: 1.466\n",
            "it: 110  | loss 5.26  | Δw: 1.472\n",
            "it: 120  | loss 5.05  | Δw: 1.433\n",
            "it: 130  | loss 4.94  | Δw: 1.424\n",
            "it: 140  | loss 4.71  | Δw: 1.381\n",
            "it: 150  | loss 4.55  | Δw: 1.362\n",
            "it: 160  | loss 4.43  | Δw: 1.348\n",
            "it: 170  | loss 4.25  | Δw: 1.324\n",
            "it: 180  | loss 4.15  | Δw: 1.316\n",
            "it: 190  | loss 4.02  | Δw: 1.307\n",
            "it: 200  | loss 3.9  | Δw: 1.303\n",
            "it: 210  | loss 3.78  | Δw: 1.292\n",
            "it: 220  | loss 3.6  | Δw: 1.245\n",
            "it: 230  | loss 3.51  | Δw: 1.273\n",
            "it: 240  | loss 3.38  | Δw: 1.247\n",
            "it: 250  | loss 3.35  | Δw: 1.247\n",
            "it: 260  | loss 3.22  | Δw: 1.251\n",
            "it: 270  | loss 3.14  | Δw: 1.211\n",
            "it: 280  | loss 3.06  | Δw: 1.213\n",
            "it: 290  | loss 2.98  | Δw: 1.207\n",
            "it: 300  | loss 2.95  | Δw: 1.239\n",
            "it: 310  | loss 2.83  | Δw: 1.208\n",
            "it: 320  | loss 2.78  | Δw: 1.205\n",
            "it: 330  | loss 2.72  | Δw: 1.192\n",
            "it: 340  | loss 2.64  | Δw: 1.182\n",
            "it: 350  | loss 2.55  | Δw: 1.147\n",
            "it: 360  | loss 2.53  | Δw: 1.156\n",
            "it: 370  | loss 2.53  | Δw: 1.175\n",
            "it: 380  | loss 2.4  | Δw: 1.136\n",
            "it: 390  | loss 2.4  | Δw: 1.171\n",
            "it: 400  | loss 2.4  | Δw: 1.175\n",
            "it: 410  | loss 2.3  | Δw: 1.127\n",
            "it: 420  | loss 2.22  | Δw: 1.116\n",
            "it: 430  | loss 2.18  | Δw: 1.103\n",
            "it: 440  | loss 2.18  | Δw: 1.126\n",
            "it: 450  | loss 2.18  | Δw: 1.086\n",
            "it: 460  | loss 2.1  | Δw: 1.1\n",
            "it: 470  | loss 2.05  | Δw: 1.071\n",
            "it: 480  | loss 2.03  | Δw: 1.06\n",
            "it: 490  | loss 2.02  | Δw: 1.07\n",
            "it: 500  | loss 1.99  | Δw: 1.065\n",
            "it: 510  | loss 2.01  | Δw: 1.064\n",
            "it: 520  | loss 1.93  | Δw: 1.049\n",
            "it: 530  | loss 1.89  | Δw: 1.035\n",
            "it: 540  | loss 1.87  | Δw: 1.038\n",
            "it: 550  | loss 1.84  | Δw: 1.028\n",
            "it: 560  | loss 1.86  | Δw: 1.046\n",
            "it: 570  | loss 1.83  | Δw: 0.998\n",
            "it: 580  | loss 1.79  | Δw: 1.004\n",
            "it: 590  | loss 1.76  | Δw: 0.98\n",
            "it: 600  | loss 1.75  | Δw: 1.002\n",
            "it: 610  | loss 1.7  | Δw: 0.98\n",
            "it: 620  | loss 1.7  | Δw: 0.974\n",
            "it: 630  | loss 1.66  | Δw: 0.961\n",
            "it: 640  | loss 1.63  | Δw: 0.943\n",
            "it: 650  | loss 1.62  | Δw: 0.924\n",
            "it: 660  | loss 1.61  | Δw: 0.913\n",
            "it: 670  | loss 1.58  | Δw: 0.929\n",
            "it: 680  | loss 1.57  | Δw: 0.912\n",
            "it: 690  | loss 1.54  | Δw: 0.868\n",
            "it: 700  | loss 1.56  | Δw: 0.883\n",
            "it: 710  | loss 1.53  | Δw: 0.896\n",
            "it: 720  | loss 1.51  | Δw: 0.865\n",
            "it: 730  | loss 1.55  | Δw: 0.888\n",
            "it: 740  | loss 1.48  | Δw: 0.852\n",
            "it: 750  | loss 1.47  | Δw: 0.83\n",
            "it: 760  | loss 1.42  | Δw: 0.816\n",
            "it: 770  | loss 1.47  | Δw: 0.837\n",
            "it: 780  | loss 1.4  | Δw: 0.805\n",
            "it: 790  | loss 1.38  | Δw: 0.828\n",
            "it: 800  | loss 1.44  | Δw: 0.825\n",
            "it: 810  | loss 1.38  | Δw: 0.791\n",
            "it: 820  | loss 1.36  | Δw: 0.811\n",
            "it: 830  | loss 1.35  | Δw: 0.767\n",
            "it: 840  | loss 1.36  | Δw: 0.802\n",
            "it: 850  | loss 1.33  | Δw: 0.797\n",
            "it: 860  | loss 1.29  | Δw: 0.764\n",
            "it: 870  | loss 1.34  | Δw: 0.79\n",
            "it: 880  | loss 1.3  | Δw: 0.774\n",
            "it: 890  | loss 1.3  | Δw: 0.749\n",
            "it: 900  | loss 1.28  | Δw: 0.735\n",
            "it: 910  | loss 1.28  | Δw: 0.745\n",
            "it: 920  | loss 1.23  | Δw: 0.71\n",
            "it: 930  | loss 1.26  | Δw: 0.724\n",
            "it: 940  | loss 1.24  | Δw: 0.718\n",
            "it: 950  | loss 1.27  | Δw: 0.716\n",
            "it: 960  | loss 1.24  | Δw: 0.693\n",
            "it: 970  | loss 1.26  | Δw: 0.712\n",
            "it: 980  | loss 1.22  | Δw: 0.678\n",
            "it: 990  | loss 1.23  | Δw: 0.688\n",
            "saving embeddings...\n",
            "end\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Model\n",
        "# =============================================================================\n",
        "#init model\n",
        "print('initializing model...')\n",
        "model = Transformer(n_code, n_heads, embed_size, inner_ff_size, len(dataset.vocab), seq_len, dropout)\n",
        "model = model.cuda()\n",
        "\n",
        "# =============================================================================\n",
        "# Optimizer\n",
        "# =============================================================================\n",
        "print('initializing optimizer and loss...')\n",
        "optimizer = optim.Adam(model.parameters(), **optim_kwargs)\n",
        "loss_model = nn.CrossEntropyLoss(ignore_index=dataset.IGNORE_IDX)\n",
        "\n",
        "# =============================================================================\n",
        "# Train\n",
        "# =============================================================================\n",
        "print('training...')\n",
        "print_each = 10\n",
        "model.train()\n",
        "batch_iter = iter(data_loader)\n",
        "n_iteration = 1000\n",
        "for it in range(n_iteration):\n",
        "    \n",
        "    #get batch\n",
        "    batch, batch_iter = get_batch(data_loader, batch_iter)\n",
        "    \n",
        "    #infer\n",
        "    masked_input = batch['input']\n",
        "    masked_target = batch['target']\n",
        "    \n",
        "    masked_input = masked_input.cuda(non_blocking=True)\n",
        "    masked_target = masked_target.cuda(non_blocking=True)\n",
        "    output = model(masked_input)\n",
        "    \n",
        "    #compute the cross entropy loss \n",
        "    output_v = output.view(-1,output.shape[-1])\n",
        "    target_v = masked_target.view(-1,1).squeeze()\n",
        "    loss = loss_model(output_v, target_v)\n",
        "    \n",
        "    #compute gradients\n",
        "    loss.backward()\n",
        "    \n",
        "    #apply gradients\n",
        "    optimizer.step()\n",
        "    \n",
        "    #print step\n",
        "    if it % print_each == 0:\n",
        "        print('it:', it, \n",
        "              ' | loss', np.round(loss.item(),2),\n",
        "              ' | Δw:', round(model.embeddings.weight.grad.abs().sum().item(),3))\n",
        "    \n",
        "    #reset gradients\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "\n",
        "# =============================================================================\n",
        "# Results analysis\n",
        "# =============================================================================\n",
        "print('saving embeddings...')\n",
        "N = 3000\n",
        "np.savetxt('values.tsv', np.round(model.embeddings.weight.detach().cpu().numpy()[0:N], 2), delimiter='\\t', fmt='%1.2f')\n",
        "s = [dataset.rvocab[i] for i in range(N)]\n",
        "open('names.tsv', 'w+').write('\\n'.join(s) )\n",
        "\n",
        "\n",
        "print('end')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate 10 input sentences taken all of same length to avoid padding\n",
        "sentences_test = ['samsung is the fourth planet from the sun and has a distinct',\n",
        "             'The bright rust color samsung is known for is due to iron-rich',\n",
        "             'Mars also has the samsung volcanoes in the solar system, Olympus Mons',\n",
        "             'When Mars is closest to the samsung its southern hemisphere is Cold',\n",
        "             'A graphic showing brief timeline of important samsung missions and the top',\n",
        "             'samsung storms are common on Mars. They can occur at any time',\n",
        "             'NASA has two other orbiters working around the samsung — the Mars',\n",
        "             'The next two craft to successfully reach the Red samsung were Mars',\n",
        "             'We now know that samsung is very cold and dry, with no',\n",
        "             'Mars once had liquid samsung on the surface and could have supported']"
      ],
      "metadata": {
        "id": "1dFl7JME9ZDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the sentences to input tensors\n",
        "inputs = torch.tensor([[dataset.vocab.get(word.lower(), 0)\n",
        "                        for word in sentence.split()] for sentence in sentences_test])\n",
        "inputs = inputs.to('cuda')\n",
        "\n",
        "# process the inputs through the model\n",
        "outputs = model(inputs)\n"
      ],
      "metadata": {
        "id": "2kp7DN1evFHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOIX4lMfLzY-",
        "outputId": "43095dc3-c835-43e6-88ce-609542b5d0e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 12, 23949])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction"
      ],
      "metadata": {
        "id": "OjmgboUDddMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate the output sentences\n",
        "output_indices = torch.argmax(outputs, dim=2).tolist()\n",
        "output_sentences = []\n",
        "for indices in output_indices:\n",
        "    words = [dataset.rvocab[index] for index in indices if index != dataset.vocab['<ignore>'] and not isinstance(index, list)]\n",
        "    sentence = ' '.join(words)\n",
        "    output_sentences.append(sentence)\n",
        "\n",
        "# print the output sentences\n",
        "for sentence in output_sentences:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-ZgklzaLzb0",
        "outputId": "03ec3ca1-c6e9-4734-c4cf-1c451cdc98ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<oov> is the fourth planet from the sun and has a (\n",
            "the bright find color <oov> is known for is due to ,\n",
            "mars also has the <oov> , in the , , olympus military\n",
            "when mars is closest to the <oov> its southern , is cold\n",
            "a , showing brief , of important <oov> missions and the top\n",
            "<oov> storms are common on , they can , at any time\n",
            ", has two other , working , the <oov> , the mars\n",
            "the next two this to successfully reach the red <oov> were mars\n",
            "we now know that <oov> is very cold and , with no\n",
            "mars once had liquid <oov> on the , and could have supported\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5tyAdNKlhs79"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b8fbfcbe0e544000e4ba3d2d9974592a7ba1a2af52205db5302ae41a0c45d995"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}