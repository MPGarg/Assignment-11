{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MPGarg/Assignment-11/blob/main/Assignment11_Part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfF0KNxIs-G7",
        "outputId": "33259d9e-2902-4674-b4c5-0c9cdb18797d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/MPGarg/Assignment-11.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwvwhXdytTj3",
        "outputId": "788fa1f9-e0b4-440e-9645-31bc39a9f9f9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Assignment-11'...\n",
            "remote: Enumerating objects: 52, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "remote: Total 52 (delta 19), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (52/52), 1.94 MiB | 1.60 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClxtPkcbtod-",
        "outputId": "f671cb04-6224-4cb6-c45e-c08aad3a4e9a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mAssignment-11\u001b[0m/  \u001b[01;34mgdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading vocab & training file\n",
        "!cp -r '/content/gdrive/MyDrive/Colab Notebooks/Assignment11/vocab.txt' /content/\n",
        "!cp -r '/content/gdrive/MyDrive/Colab Notebooks/Assignment11/training.txt' /content/"
      ],
      "metadata": {
        "id": "UqjWAVRBt9Jg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3iAVOAat9MX",
        "outputId": "dd7f40f5-ecb0-48d6-a2f8-23c126c44346"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mAssignment-11\u001b[0m/  \u001b[01;34mgdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/  training.txt  vocab.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoLbyA1qvWRc",
        "outputId": "8d60a3bf-0728-496a-a971-1053dc35942a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "cYq3Smi6sn_S"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Libs\n",
        "# =============================================================================\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "from os.path import exists\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import math\n",
        "import re\n",
        "#from random import *\n",
        "\n",
        "# =============================================================================\n",
        "# Transformer\n",
        "# =============================================================================\n",
        "def attention(q, k, v, mask = None, dropout = None):\n",
        "    scores = q.matmul(k.transpose(-2, -1))\n",
        "    scores /= math.sqrt(q.shape[-1])\n",
        "    \n",
        "    #mask\n",
        "    scores = scores if mask is None else scores.masked_fill(mask == 0, -1e3)\n",
        "    \n",
        "    scores = F.softmax(scores, dim = -1)\n",
        "    scores = dropout(scores) if dropout is not None else scores\n",
        "    output = scores.matmul(v)\n",
        "    return output\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_heads, out_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "#        self.q_linear = nn.Linear(out_dim, out_dim)\n",
        "#        self.k_linear = nn.Linear(out_dim, out_dim)\n",
        "#        self.v_linear = nn.Linear(out_dim, out_dim)\n",
        "        self.linear = nn.Linear(out_dim, out_dim*3)\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "        self.out_dim = out_dim\n",
        "        self.out_dim_per_head = out_dim // n_heads\n",
        "        self.out = nn.Linear(out_dim, out_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def split_heads(self, t):\n",
        "        return t.reshape(t.shape[0], -1, self.n_heads, self.out_dim_per_head)\n",
        "    \n",
        "    def forward(self, x, y=None, mask=None):\n",
        "        #in decoder, y comes from encoder. In encoder, y=x\n",
        "        y = x if y is None else y\n",
        "        \n",
        "        qkv = self.linear(x) # BS * SEQ_LEN * (3*EMBED_SIZE_L)\n",
        "        q = qkv[:, :, :self.out_dim] # BS * SEQ_LEN * EMBED_SIZE_L\n",
        "        k = qkv[:, :, self.out_dim:self.out_dim*2] # BS * SEQ_LEN * EMBED_SIZE_L\n",
        "        v = qkv[:, :, self.out_dim*2:] # BS * SEQ_LEN * EMBED_SIZE_L\n",
        "        \n",
        "        #break into n_heads\n",
        "        q, k, v = [self.split_heads(t) for t in (q,k,v)]  # BS * SEQ_LEN * HEAD * EMBED_SIZE_P_HEAD\n",
        "        q, k, v = [t.transpose(1,2) for t in (q,k,v)]  # BS * HEAD * SEQ_LEN * EMBED_SIZE_P_HEAD\n",
        "        \n",
        "        #n_heads => attention => merge the heads => mix information\n",
        "        scores = attention(q, k, v, mask, self.dropout) # BS * HEAD * SEQ_LEN * EMBED_SIZE_P_HEAD\n",
        "        scores = scores.transpose(1,2).contiguous().view(scores.shape[0], -1, self.out_dim) # BS * SEQ_LEN * EMBED_SIZE_L\n",
        "        out = self.out(scores)  # BS * SEQ_LEN * EMBED_SIZE\n",
        "        \n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, inp_dim, inner_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(inp_dim, inner_dim)\n",
        "        self.linear2 = nn.Linear(inner_dim, inp_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        #inp => inner => relu => dropout => inner => inp\n",
        "        return self.linear2(self.dropout(F.relu(self.linear1(x)))) \n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, n_heads, inner_transformer_size, inner_ff_size, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(n_heads, inner_transformer_size, dropout)\n",
        "        self.ff = FeedForward(inner_transformer_size, inner_ff_size, dropout)\n",
        "        self.norm1 = nn.LayerNorm(inner_transformer_size)\n",
        "        self.norm2 = nn.LayerNorm(inner_transformer_size)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        x2 = self.norm1(x)\n",
        "        x = x + self.dropout1(self.mha(x2, mask=mask))\n",
        "        x2 = self.norm2(x)\n",
        "        x = x + self.dropout2(self.ff(x2))\n",
        "        return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, n_code, n_heads, embed_size, inner_ff_size, n_embeddings, seq_len, dropout=.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        #model input\n",
        "        self.embeddings = nn.Embedding(n_embeddings, embed_size)\n",
        "        self.pe = PositionalEmbedding(embed_size, seq_len)\n",
        "        \n",
        "        #backbone\n",
        "        encoders = []\n",
        "        for i in range(n_code):\n",
        "            encoders += [EncoderLayer(n_heads, embed_size, inner_ff_size, dropout)]\n",
        "        self.encoders = nn.ModuleList(encoders)\n",
        "        \n",
        "        #language model\n",
        "        self.norm = nn.LayerNorm(embed_size)\n",
        "        self.linear = nn.Linear(embed_size, n_embeddings, bias=False)\n",
        "                \n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.embeddings(x)\n",
        "        x = x + self.pe(x)\n",
        "        for encoder in self.encoders:\n",
        "            x = encoder(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "# Positional Embedding\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len = 80):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        pe.requires_grad = False\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.pe[:,:x.size(1)] #x.size(1) = seq_len\n",
        "    \n",
        "# =============================================================================\n",
        "# Dataset\n",
        "# =============================================================================\n",
        "class SentencesDataset(Dataset):\n",
        "    #Init dataset\n",
        "    def __init__(self, sentences, vocab, seq_len):\n",
        "        dataset = self\n",
        "        \n",
        "        dataset.sentences = sentences\n",
        "        dataset.vocab = vocab + ['<ignore>', '<oov>', '<mask>']\n",
        "        dataset.vocab_size = len(vocab)\n",
        "        dataset.vocab = {e:i for i, e in enumerate(dataset.vocab)} \n",
        "        dataset.rvocab = {v:k for k,v in dataset.vocab.items()}\n",
        "        dataset.seq_len = seq_len\n",
        "        \n",
        "        #special tags\n",
        "        dataset.IGNORE_IDX = dataset.vocab['<ignore>'] #replacement tag for tokens to ignore\n",
        "        dataset.OUT_OF_VOCAB_IDX = dataset.vocab['<oov>'] #replacement tag for unknown words\n",
        "        dataset.MASK_IDX = dataset.vocab['<mask>'] #replacement tag for the masked word prediction task       \n",
        "    \n",
        "    #fetch data\n",
        "    def __getitem__(self, index, p_random_mask=0.15):\n",
        "        dataset = self\n",
        "        \n",
        "        #while we don't have enough word to fill the sentence for a batch\n",
        "        s = []\n",
        "        while len(s) < dataset.seq_len:\n",
        "            s.extend(dataset.get_sentence_idx(index % len(dataset)))\n",
        "            index += 1\n",
        "        \n",
        "        #ensure that the sequence is of length seq_len\n",
        "        s = s[:dataset.seq_len]\n",
        "        [s.append(dataset.IGNORE_IDX) for i in range(dataset.seq_len - len(s))] #PAD ok\n",
        "        \n",
        "        #apply random word \n",
        "        s1 = []\n",
        "        for w in s:\n",
        "          if random.random() < p_random_mask:\n",
        "              prob = random.random()\n",
        "              if prob < 0.8:\n",
        "                  st = [dataset.MASK_IDX, w]\n",
        "                  s1.append(st)                  \n",
        "              elif prob < 0.9:\n",
        "                  #any random word 10% of 15%\n",
        "                  st = [dataset.vocab[dataset.rvocab[random.randrange(dataset.vocab_size)]], w]\n",
        "                  s1.append(st)\n",
        "              else:\n",
        "                  #same word 10% of 15%\n",
        "                  st = [w,w]\n",
        "                  s1.append(st)\n",
        "          else:\n",
        "              st = [w, dataset.IGNORE_IDX]\n",
        "              s1.append(st)\n",
        "        \n",
        "        #s = [((dataset.MASK_IDX, w) if random.random() < 0.8 elif (dataset.vocab[dataset.rvocab[randint(0, dataset.vocab_size - 1)]],w)) \n",
        "        #    if random.random() < p_random_mask else (w, dataset.IGNORE_IDX) for w in s]\n",
        "        \n",
        "        s = s1\n",
        "\n",
        "        return {'input': torch.Tensor([w[0] for w in s]).long(),\n",
        "                'target': torch.Tensor([w[1] for w in s]).long()}\n",
        "\n",
        "    #return length\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    #get words id\n",
        "    def get_sentence_idx(self, index):\n",
        "        dataset = self\n",
        "        s = dataset.sentences[index]\n",
        "        s = [dataset.vocab[w] if w in dataset.vocab else dataset.OUT_OF_VOCAB_IDX for w in s] \n",
        "        return s\n",
        "\n",
        "# =============================================================================\n",
        "# Methods / Class\n",
        "# =============================================================================\n",
        "def get_batch(loader, loader_iter):\n",
        "    try:\n",
        "        batch = next(loader_iter)\n",
        "    except StopIteration:\n",
        "        loader_iter = iter(loader)\n",
        "        batch = next(loader_iter)\n",
        "    return batch, loader_iter\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# #Init\n",
        "# =============================================================================\n",
        "print('initializing..')\n",
        "batch_size = 1024\n",
        "seq_len = 20\n",
        "embed_size = 128\n",
        "inner_ff_size = embed_size * 4\n",
        "n_heads = 8\n",
        "n_code = 8\n",
        "n_vocab = 40000\n",
        "dropout = 0.1\n",
        "# n_workers = 12\n",
        "\n",
        "#optimizer\n",
        "optim_kwargs = {'lr':1e-4, 'weight_decay':1e-4, 'betas':(.9,.999)}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REytZ7yCUzNz",
        "outputId": "fbd05f39-8d01-4c22-d595-f0cb1cc3ad28"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initializing..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Input\n",
        "# =============================================================================\n",
        "#1) load text\n",
        "print('loading text...')\n",
        "pth = 'training.txt'\n",
        "sentences = open(pth).read().lower().split('\\n')\n",
        "\n",
        "#2) tokenize sentences (can be done during training, you can also use spacy udpipe)\n",
        "print('tokenizing sentences...')\n",
        "special_chars = ',?;.:/*!+-()[]{}\"\\'&'\n",
        "sentences = [re.sub(f'[{re.escape(special_chars)}]', ' \\g<0> ', s).split(' ') for s in sentences]\n",
        "sentences = [[w for w in s if len(w)] for s in sentences]\n",
        "\n",
        "#3) create vocab if not already created\n",
        "print('creating/loading vocab...')\n",
        "pth = 'vocab.txt'\n",
        "if not exists(pth):\n",
        "    words = [w for s in sentences for w in s]\n",
        "    vocab = Counter(words).most_common(n_vocab) #keep the N most frequent words\n",
        "    vocab = [w[0] for w in vocab]\n",
        "    open(pth, 'w+').write('\\n'.join(vocab))\n",
        "else:\n",
        "    vocab = open(pth).read().split('\\n')\n",
        "\n",
        "#4) create dataset\n",
        "print('creating dataset...')\n",
        "dataset = SentencesDataset(sentences, vocab, seq_len)\n",
        "# kwargs = {'num_workers':n_workers, 'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
        "kwargs = {'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
        "data_loader = torch.utils.data.DataLoader(dataset, **kwargs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HU-RAqXtUzQn",
        "outputId": "34eb45c6-c02d-4727-a543-51fc308be68f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading text...\n",
            "tokenizing sentences...\n",
            "creating/loading vocab...\n",
            "creating dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBurNy4hsn_Y",
        "outputId": "d2e32e8c-92e4-490f-b910-f52163a17f6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initializing model...\n",
            "initializing optimizer and loss...\n",
            "training...\n",
            "it: 0  | loss 10.22  | Δw: 2.272\n",
            "it: 20  | loss 8.44  | Δw: 0.652\n",
            "it: 40  | loss 8.0  | Δw: 0.449\n",
            "it: 60  | loss 7.67  | Δw: 0.394\n",
            "it: 80  | loss 7.19  | Δw: 0.364\n",
            "it: 100  | loss 6.92  | Δw: 0.385\n",
            "it: 120  | loss 6.61  | Δw: 0.392\n",
            "it: 140  | loss 6.29  | Δw: 0.418\n",
            "it: 160  | loss 6.0  | Δw: 0.448\n",
            "it: 180  | loss 5.7  | Δw: 0.407\n",
            "it: 200  | loss 5.45  | Δw: 0.419\n",
            "it: 220  | loss 5.39  | Δw: 0.472\n",
            "it: 240  | loss 5.21  | Δw: 0.437\n",
            "it: 260  | loss 5.06  | Δw: 0.468\n",
            "it: 280  | loss 5.11  | Δw: 0.484\n",
            "it: 300  | loss 4.89  | Δw: 0.484\n",
            "it: 320  | loss 4.99  | Δw: 0.513\n",
            "it: 340  | loss 4.9  | Δw: 0.545\n",
            "it: 360  | loss 4.72  | Δw: 0.572\n",
            "it: 380  | loss 4.78  | Δw: 0.653\n",
            "it: 400  | loss 4.81  | Δw: 0.597\n",
            "it: 420  | loss 4.73  | Δw: 0.658\n",
            "it: 440  | loss 4.74  | Δw: 0.644\n",
            "it: 460  | loss 4.81  | Δw: 0.653\n",
            "it: 480  | loss 4.79  | Δw: 0.718\n",
            "it: 500  | loss 4.74  | Δw: 0.8\n",
            "it: 520  | loss 4.66  | Δw: 0.755\n",
            "it: 540  | loss 4.79  | Δw: 0.799\n",
            "it: 560  | loss 4.71  | Δw: 0.866\n",
            "it: 580  | loss 4.65  | Δw: 0.804\n",
            "it: 600  | loss 4.62  | Δw: 0.867\n",
            "it: 620  | loss 4.63  | Δw: 0.994\n",
            "it: 640  | loss 4.67  | Δw: 0.948\n",
            "it: 660  | loss 4.66  | Δw: 0.922\n",
            "it: 680  | loss 4.54  | Δw: 1.063\n",
            "it: 700  | loss 4.54  | Δw: 1.018\n",
            "it: 720  | loss 4.59  | Δw: 1.006\n",
            "it: 740  | loss 4.66  | Δw: 1.035\n",
            "it: 760  | loss 4.71  | Δw: 1.212\n",
            "it: 780  | loss 4.55  | Δw: 1.201\n",
            "it: 800  | loss 4.6  | Δw: 1.194\n",
            "it: 820  | loss 4.47  | Δw: 1.161\n",
            "it: 840  | loss 4.61  | Δw: 1.236\n",
            "it: 860  | loss 4.59  | Δw: 1.29\n",
            "it: 880  | loss 4.61  | Δw: 1.249\n",
            "it: 900  | loss 4.6  | Δw: 1.315\n",
            "it: 920  | loss 4.54  | Δw: 1.205\n",
            "it: 940  | loss 4.51  | Δw: 1.32\n",
            "it: 960  | loss 4.44  | Δw: 1.361\n",
            "it: 980  | loss 4.53  | Δw: 1.39\n",
            "it: 1000  | loss 4.43  | Δw: 1.4\n",
            "it: 1020  | loss 4.44  | Δw: 1.505\n",
            "it: 1040  | loss 4.5  | Δw: 1.444\n",
            "it: 1060  | loss 4.68  | Δw: 1.423\n",
            "it: 1080  | loss 4.56  | Δw: 1.447\n",
            "it: 1100  | loss 4.44  | Δw: 1.525\n",
            "it: 1120  | loss 4.49  | Δw: 1.483\n",
            "it: 1140  | loss 4.47  | Δw: 1.519\n",
            "it: 1160  | loss 4.34  | Δw: 1.593\n",
            "it: 1180  | loss 4.41  | Δw: 1.556\n",
            "it: 1200  | loss 4.48  | Δw: 1.638\n",
            "it: 1220  | loss 4.35  | Δw: 1.574\n",
            "it: 1240  | loss 4.54  | Δw: 1.543\n",
            "it: 1260  | loss 4.51  | Δw: 1.801\n",
            "it: 1280  | loss 4.44  | Δw: 1.561\n",
            "it: 1300  | loss 4.46  | Δw: 1.898\n",
            "it: 1320  | loss 4.53  | Δw: 1.553\n",
            "it: 1340  | loss 4.4  | Δw: 1.652\n",
            "it: 1360  | loss 4.33  | Δw: 1.78\n",
            "it: 1380  | loss 4.46  | Δw: 1.65\n",
            "it: 1400  | loss 4.46  | Δw: 1.769\n",
            "it: 1420  | loss 4.33  | Δw: 1.692\n",
            "it: 1440  | loss 4.41  | Δw: 1.778\n",
            "it: 1460  | loss 4.35  | Δw: 1.761\n",
            "it: 1480  | loss 4.38  | Δw: 1.722\n",
            "it: 1500  | loss 4.47  | Δw: 1.821\n",
            "it: 1520  | loss 4.34  | Δw: 1.913\n",
            "it: 1540  | loss 4.25  | Δw: 1.803\n",
            "it: 1560  | loss 4.34  | Δw: 2.362\n",
            "it: 1580  | loss 4.33  | Δw: 1.996\n",
            "it: 1600  | loss 4.4  | Δw: 2.023\n",
            "it: 1620  | loss 4.36  | Δw: 1.884\n",
            "it: 1640  | loss 4.23  | Δw: 2.119\n",
            "it: 1660  | loss 4.39  | Δw: 2.287\n",
            "it: 1680  | loss 4.27  | Δw: 2.187\n",
            "it: 1700  | loss 4.35  | Δw: 2.386\n",
            "it: 1720  | loss 4.34  | Δw: 2.343\n",
            "it: 1740  | loss 4.28  | Δw: 2.199\n",
            "it: 1760  | loss 4.21  | Δw: 2.131\n",
            "it: 1780  | loss 4.3  | Δw: 2.376\n",
            "it: 1800  | loss 4.32  | Δw: 2.243\n",
            "it: 1820  | loss 4.22  | Δw: 2.182\n",
            "it: 1840  | loss 4.27  | Δw: 2.508\n",
            "it: 1860  | loss 4.3  | Δw: 2.338\n",
            "it: 1880  | loss 4.24  | Δw: 2.304\n",
            "it: 1900  | loss 4.24  | Δw: 2.402\n",
            "it: 1920  | loss 4.2  | Δw: 2.394\n",
            "it: 1940  | loss 4.2  | Δw: 2.607\n",
            "it: 1960  | loss 4.23  | Δw: 2.604\n",
            "it: 1980  | loss 4.27  | Δw: 2.513\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Model\n",
        "# =============================================================================\n",
        "#init model\n",
        "print('initializing model...')\n",
        "model = Transformer(n_code, n_heads, embed_size, inner_ff_size, len(dataset.vocab), seq_len, dropout)\n",
        "model = model.cuda()\n",
        "\n",
        "# =============================================================================\n",
        "# Optimizer\n",
        "# =============================================================================\n",
        "print('initializing optimizer and loss...')\n",
        "optimizer = optim.Adam(model.parameters(), **optim_kwargs)\n",
        "loss_model = nn.CrossEntropyLoss(ignore_index=dataset.IGNORE_IDX)\n",
        "\n",
        "# =============================================================================\n",
        "# Train\n",
        "# =============================================================================\n",
        "print('training...')\n",
        "print_each = 20\n",
        "model.train()\n",
        "batch_iter = iter(data_loader)\n",
        "n_iteration = 2000\n",
        "for it in range(n_iteration):\n",
        "    \n",
        "    #get batch\n",
        "    batch, batch_iter = get_batch(data_loader, batch_iter)\n",
        "    \n",
        "    #infer\n",
        "    masked_input = batch['input']\n",
        "    masked_target = batch['target']\n",
        "    \n",
        "    masked_input = masked_input.cuda(non_blocking=True)\n",
        "    masked_target = masked_target.cuda(non_blocking=True)\n",
        "    output = model(masked_input)\n",
        "    \n",
        "    #compute the cross entropy loss \n",
        "    output_v = output.view(-1,output.shape[-1])\n",
        "    target_v = masked_target.view(-1,1).squeeze()\n",
        "    loss = loss_model(output_v, target_v)\n",
        "    \n",
        "    #compute gradients\n",
        "    loss.backward()\n",
        "    \n",
        "    #apply gradients\n",
        "    optimizer.step()\n",
        "    \n",
        "    #print step\n",
        "    if it % print_each == 0:\n",
        "        print('it:', it, \n",
        "              ' | loss', np.round(loss.item(),2),\n",
        "              ' | Δw:', round(model.embeddings.weight.grad.abs().sum().item(),3))\n",
        "    \n",
        "    #reset gradients\n",
        "    optimizer.zero_grad()\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Results analysis\n",
        "# =============================================================================\n",
        "print('saving embeddings...')\n",
        "N = 3000\n",
        "np.savetxt('values.tsv', np.round(model.embeddings.weight.detach().cpu().numpy()[0:N], 2), delimiter='\\t', fmt='%1.2f')\n",
        "s = [dataset.rvocab[i] for i in range(N)]\n",
        "open('names.tsv', 'w+').write('\\n'.join(s) )\n",
        "\n",
        "\n",
        "print('end')"
      ],
      "metadata": {
        "id": "A-whjIHx5lK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate 10 input sentences taken all of same length to avoid padding\n",
        "sentences_test = ['<mask> is the fourth planet from the sun and has a distinct',\n",
        "             'The bright rust color <mask> is known for is due to iron-rich',\n",
        "             'Mars also has the <mask> volcanoes in the solar system, Olympus Mons',\n",
        "             'When Mars is closest to the <mask> its southern hemisphere is Cold',\n",
        "             'A graphic showing brief timeline of important <mask> missions and the top',\n",
        "             '<mask> storms are common on Mars. They can occur at any time',\n",
        "             'NASA has two other orbiters working around the <mask> — the Mars',\n",
        "             'The next two craft to successfully reach the Red <mask> were Mars',\n",
        "             'We now know that <mask> is very cold and dry, with no',\n",
        "             'Mars once had liquid <mask> on the surface and could have supported']"
      ],
      "metadata": {
        "id": "1dFl7JME9ZDj"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the sentences to input tensors\n",
        "inputs = torch.tensor([[dataset.vocab.get(word.lower(), 0)\n",
        "                        for word in sentence.split()] for sentence in sentences_test])\n",
        "inputs = inputs.to('cuda')\n",
        "\n",
        "# process the inputs through the model\n",
        "outputs = model(inputs)\n"
      ],
      "metadata": {
        "id": "2kp7DN1evFHX"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction"
      ],
      "metadata": {
        "id": "OjmgboUDddMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate the output sentences\n",
        "output_indices = torch.argmax(outputs, dim=2).tolist()\n",
        "output_sentences = []\n",
        "for indices in output_indices:\n",
        "    words = [dataset.rvocab[index] for index in indices if index != dataset.vocab['<ignore>'] and not isinstance(index, list)]\n",
        "    sentence = ' '.join(words)\n",
        "    output_sentences.append(sentence)\n",
        "\n",
        "# print the output sentences\n",
        "for sentence in output_sentences:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-ZgklzaLzb0",
        "outputId": "6dcb0142-e821-4776-a49a-5331840ecdad"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<oov> is the <oov> <oov> from the <oov> and <oov> a <oov>\n",
            "the <oov> <oov> <oov> <oov> is <oov> for is <oov> to ,\n",
            "<oov> also <oov> the <oov> , in the , , <oov> <oov>\n",
            "<oov> <oov> is <oov> to the <oov> <oov> <oov> , is <oov>\n",
            "a , <oov> <oov> , of <oov> <oov> <oov> and the <oov>\n",
            "<oov> <oov> are <oov> on , they <oov> , at <oov> time\n",
            ", <oov> <oov> <oov> , <oov> , the of , the <oov>\n",
            "the <oov> <oov> <oov> to <oov> <oov> the <oov> of were <oov>\n",
            "we <oov> <oov> that <oov> is <oov> <oov> and , with <oov>\n",
            "<oov> <oov> had <oov> <oov> on the , and . have the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5tyAdNKlhs79"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b8fbfcbe0e544000e4ba3d2d9974592a7ba1a2af52205db5302ae41a0c45d995"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}